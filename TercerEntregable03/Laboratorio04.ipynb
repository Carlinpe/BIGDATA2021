{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio 04\n",
        "# PySpark\n",
        "\n",
        "```\n",
        "Curso     : Minería de Datos 2021-2\n",
        "Docente   : Carlos Fernando Montoya Cubas\n",
        "Autor     : Carlos Enrique Quispe Chambilla\n",
        "Lugar     : Cusco, Perú, 2022.\n",
        "```"
      ],
      "metadata": {
        "id": "ejTVjmFOKJT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalacion de PySpark"
      ],
      "metadata": {
        "id": "hxvwD3h9KLan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0-oCLhxKN3j",
        "outputId": "f116467b-2364-4b4b-8c7c-80e6df2ca3f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 35 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 45.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=46a4e1d3c6a522d25f395a33c13d4a84be2675c0727539d719a5def41ece6a48\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "nMDelleGKVFM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP3jFHtK72eH"
      },
      "source": [
        "# **Práctica de laboratorio 5b: k-Means para cuantificar atributos**\n",
        "\n",
        "#### Los algoritmos de agrupación de datos, además de utilizarse en el análisis exploratorio para extraer patrones de similitud entre objetos, pueden utilizarse para comprimir el espacio de datos.\n",
        "\n",
        "#### En este notebook usaremos nuestra base de datos Sentiment Movie Reviews para los experimentos. Primero usaremos la técnica word2vec que aprende una transformación de tokens desde una base a un vector de atributos. A continuación, utilizaremos el algoritmo k-Means para comprimir la información sobre estos atributos y proyectar cada objeto en un espacio de atributos de tamaño fijo.\n",
        "\n",
        "#### Las celdas de ejercicio comienzan con el comentario `# EJERCICIO` y los códigos a completar están marcados con los comentarios `<COMPLETO>`.\n",
        "\n",
        "#### ** En este notebook: **\n",
        "#### *Parte 1:* Word2Vec\n",
        "#### *Parte 2:* k-Means para cuantificar atributos\n",
        "#### *Parte 3:* Aplicar un k-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwqr_jL272ea"
      },
      "source": [
        "### **Parte 0: Preliminares**\n",
        "\n",
        "#### Para este notebook usaremos la base de datos de reseñas de películas que se usará para el segundo proyecto.\n",
        "\n",
        "#### La base de datos tiene los campos separados por '\\t' y el siguiente formato:\n",
        "    `\"id de frase\",\"id de oración\",\"Frase\",\"Sentimiento\"`\n",
        "\n",
        "#### Para esta práctica de laboratorio solo usaremos el campo \"Frase\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "Rf6Rv6lj72ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b137f9-5626-4c49-80cb-4dde06aa5334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 8528 lines\n",
            "Sample line: (8060, \"The band performances featured in Drumline are red hot ... -LRB- but -RRB- from a mere story point of view , the film 's ice cold .\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def parseRDD(point):\n",
        "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
        "        a sentence (third field).\n",
        "    Args:\n",
        "        point (str): input data point\n",
        "    Returns:\n",
        "        str: a string\n",
        "    \"\"\"    \n",
        "    data = point.split('\\t')\n",
        "    return (int(data[0]),data[2])\n",
        "\n",
        "def notempty(point):\n",
        "    \"\"\" Returns whether the point string is not empty\n",
        "    Args:\n",
        "        point (str): input string\n",
        "    Returns:\n",
        "        bool: True if it is not empty\n",
        "    \"\"\"   \n",
        "    return len(point[1])>0\n",
        "\n",
        "filename = os.path.join(\"Data\",\"/content/MovieReviews2.tsv\")\n",
        "rawRDD = sc.textFile(filename,100)\n",
        "header = rawRDD.take(1)[0]\n",
        "\n",
        "dataRDD = (rawRDD\n",
        "           #.sample(False, 0.1, seed=42)\n",
        "           .filter(lambda x: x!=header)\n",
        "           .map(parseRDD)\n",
        "           .filter(notempty)\n",
        "           #.sample( False, 0.1, 42 )\n",
        "           )\n",
        "\n",
        "print ('Read {} lines'.format(dataRDD.count()))\n",
        "print ('Sample line: {}'.format(dataRDD.takeSample(False, 1)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmo7pEFF72ei"
      },
      "source": [
        "### **Parte 1: Word2Vec**\n",
        "\n",
        "#### La técnica [word2vec][word2vec] aprende a través de una red neuronal semántica una representación vectorial de cada token en un corpus de tal manera que las palabras semánticamente similares son similares en la representación vectorial.\n",
        "\n",
        "#### PySpark contiene una implementación de esta técnica, para aplicarla basta con pasar un RDD en el que cada objeto representa un documento y cada documento está representado por una lista de tokens en el orden en que aparecen originalmente en el corpus. Después del proceso de entrenamiento, podemos transformar un token usando el método [`transform`](https://spark.apache.org/docs/latest/ml-features) para convertir cada token en una representación vectorial.\n",
        "\n",
        "#### En este punto, cada objeto en nuestra base estará representado por una matriz de tamaño variable.\n",
        "\n",
        "[word2vec]: https://code.google.com/p/word2vec/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2231KtDk72ek"
      },
      "source": [
        "### **(1a) Generación de RDD a partir de tokens**\n",
        "\n",
        "#### Use la función de tokenización `tokenize` para generar un RDD `wordsRDD` que contenga listas de tokens de nuestra base de datos original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "Sh0bA8n_72em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ef11d9-f1bf-4605-e2aa-3c78bee75a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quiet', 'introspective', 'entertaining', 'independent', 'worth', 'seeking']\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "import re\n",
        "\n",
        "split_regex = r'\\W+'\n",
        "\n",
        "stopfile = os.path.join(\"Data\",\"/content/stopwords.txt\")\n",
        "stopwords = set((sc.textFile(stopfile)).collect())\n",
        "\n",
        "def tokenize(string):\n",
        "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
        "    Args:\n",
        "        string (str): input string\n",
        "    Returns:\n",
        "        list: a list of tokens without stopwords\n",
        "    \"\"\"\n",
        "    str_list = re.split(split_regex, string)\n",
        "    str_list = filter(lambda w: len(w)>0, map(lambda w: w.lower(), str_list))\n",
        "    return [w for w in str_list if w not in stopwords]\n",
        "\n",
        "wordsRDD = dataRDD.map(lambda x: tokenize(x[1]))\n",
        "\n",
        "print (wordsRDD.take(1)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "yq_IKuWF72er"
      },
      "outputs": [],
      "source": [
        "# TEST Tokenize a String (1a)\n",
        "assert wordsRDD.take(1)[0]==[u'quiet', u'introspective', u'entertaining', u'independent', u'worth', u'seeking'], 'lista incorreta!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH1PyHba72et"
      },
      "source": [
        "### **(1b) Aplicando la transformación word2vec**\n",
        "\n",
        "#### Cree una plantilla word2vec aplicando el método `fit` al RDD creado en el ejercicio anterior.\n",
        "\n",
        "#### Para aplicar este método debes hacer un pipeline de métodos, primero ejecutando `Word2Vec()`, luego aplicando el método `setVectorSize()` con el tamaño que queremos para nuestro vector (usa el tamaño 5), seguido de ` setSeed()` para la semilla aleatoria, en caso de experimentos controlados (usaremos 42) y finalmente `fit()` con nuestro `wordsRDD` como parámetro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "6ZVz5auF72ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3cbe03-2517-4aec-bb6d-89f5b5dbb6ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]\n",
            "[('cgi', 0.989105761051178), ('something', 0.9889155626296997)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "from pyspark.mllib.feature import Word2Vec\n",
        "\n",
        "model = (Word2Vec()\n",
        "         .setVectorSize(5)\n",
        "         .setSeed(42)\n",
        "         .fit(wordsRDD))\n",
        "\n",
        "print (model.transform(u'entertaining'))\n",
        "print (list(model.findSynonyms(u'entertaining', 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "ju-ZmUeV72ex"
      },
      "outputs": [],
      "source": [
        "dist = np.abs(model.transform(u'entertaining')-np.array([-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659])).mean()\n",
        "assert dist<1e-6, 'valores incorretos'\n",
        "assert list(model.findSynonyms(u'entertaining', 1))[0][0] == 'cgi', 'valores incorretos'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWJQBSRJ72ey"
      },
      "source": [
        "### **(1c) Generando un RDD de arreglos**\n",
        "\n",
        "#### Como primer paso, necesitamos generar un diccionario donde la clave son las palabras y el valor es el vector que representa esa palabra.\n",
        "\n",
        "#### Para esto primero generaremos una lista `uniqueWords` que contiene las palabras únicas de las palabras RDD, eliminando aquellas que aparecen menos de 5 veces [$^1$](#1). A continuación, crearemos un diccionario `w2v` donde la clave es un token y el valor es un `np.array` del arreglo transformado de ese token[$^2$](#2).\n",
        "\n",
        "#### Finalmente, creemos un RDD llamado `vectorsRDD` donde cada registro está representado por una matriz donde cada fila representa una palabra transformada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "2BVExHAZ72e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49952439-81b8-42a9-bbe5-da12b68124dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3388 tokens únicos\n",
            "Vetor entertaining: [-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]\n",
            "(5, 5) (10, 5)\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "uniqueWords = (wordsRDD\n",
        "               .flatMap(lambda ws: [(w, 1) for w in ws])\n",
        "               .reduceByKey(lambda x,y: x+y)\n",
        "               .filter(lambda wf: wf[1]>=5)\n",
        "               .map(lambda wf: wf[0])\n",
        "               .collect()\n",
        "               )\n",
        "\n",
        "print ('{} tokens únicos'.format(len(uniqueWords)))\n",
        "\n",
        "w2v = {}\n",
        "for w in uniqueWords:\n",
        "    w2v[w] = model.transform(w)\n",
        "w2vb = sc.broadcast(w2v)       \n",
        "print ('Vetor entertaining: {}'.format( w2v[u'entertaining']))\n",
        "\n",
        "vectorsRDD = (wordsRDD\n",
        "              .map(lambda ws: np.array([w2vb.value[w] for w in ws if w in w2vb.value]))\n",
        "             )\n",
        "recs = vectorsRDD.take(2)\n",
        "firstRec, secondRec = recs[0], recs[1]\n",
        "print (firstRec.shape, secondRec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "SjqZYy5s72e1"
      },
      "outputs": [],
      "source": [
        "# TEST Tokenizing the small datasets (1c)\n",
        "assert len(uniqueWords) == 3388,  'valor incorreto'\n",
        "assert np.mean(np.abs(w2v[u'entertaining']-[-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]))<1e-6,'valor incorreto'\n",
        "assert secondRec.shape == (10,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okvGdbNg72e3"
      },
      "source": [
        "### **Parte 2: k-Means para cuantificar atributos**\n",
        "\n",
        "#### Llegados a este punto, es fácil ver que no podemos aplicar nuestras técnicas de aprendizaje supervisado a esta base de datos:\n",
        "\n",
        "   * #### La regresión logística requiere un vector de tamaño fijo que represente cada objeto\n",
        "   * #### k-NN necesita una forma clara de comparar dos objetos, ¿qué métrica de similitud debemos aplicar?\n",
        "  \n",
        "#### Para resolver esta situación, realicemos una nueva transformación en nuestro RDD. Primero, aprovechemos el hecho de que dos tokens con un significado similar se asignan a vectores similares para agruparlos en un solo atributo.\n",
        "\n",
        "#### Al aplicar k-Means a este conjunto de vectores, podemos crear $k$ puntos representativos y, para cada documento, generar un histograma de recuento de tokens en los clústeres generados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvGWBBWQ72e5"
      },
      "source": [
        "#### **(2a) Agrupando los vectores y creando centros representativos**\n",
        "\n",
        "#### Como primer paso generaremos un RDD con los valores del diccionario `w2v`. A continuación, aplicaremos el algoritmo `k-Means` con $k = 200$ y $seed = 42$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "UEq4uFDq72e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db657591-022c-4c4c-b937-b7c7aa7bee22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample vector: [array([-0.07269461,  0.09603201,  0.20506908, -0.03772384,  0.08151765])]\n",
            "10 first clusters allocation: [5, 136, 37, 12, 145, 66, 63, 84, 140, 66]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "from  pyspark.mllib.clustering import KMeans\n",
        "\n",
        "vectors2RDD = sc.parallelize(np.array(list(w2v.values())),1)\n",
        "print ('Sample vector: {}'.format(vectors2RDD.take(1)))\n",
        "\n",
        "modelK = KMeans.train(vectors2RDD, 200, seed=42)\n",
        "\n",
        "clustersRDD = vectors2RDD.map(lambda x: modelK.predict(x))\n",
        "print ('10 first clusters allocation: {}'.format(clustersRDD.take(10)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "2az8-uNI72e8"
      },
      "outputs": [],
      "source": [
        "# TEST Amazon record with the most tokens (1d)\n",
        "assert clustersRDD.take(10)==[5, 136, 37, 12, 145, 66, 63, 84, 140, 66], 'valor incorreto'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV_Xy41j72e9"
      },
      "source": [
        "#### **(2b) Transformación de matriz de datos en vectores cuantificados**\n",
        "\n",
        "#### El siguiente paso es transformar nuestro RDD de frases en un RDD de pares (id, vector cuantificado). Para ello crearemos una función cuantificadora que recibirá como parámetros el objeto, el modelo k-means, el valor de k y el diccionario word2vec.\n",
        "\n",
        "#### Para cada punto, separemos el id y apliquemos la función `tokenize` a la cadena. Luego transformamos la lista de tokens en una matriz word2vec. Finalmente, aplicamos cada vector de esta matriz al modelo k-Means, generando un vector de tamaño $k$ donde cada posición $i$ indica cuántos tokens pertenecen al clúster $i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "Ox-Zdvyn72e-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51625652-1a73-47b3-e247-1e41a0832c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(64, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0.]))]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "def quantizador(point, model, k, w2v):\n",
        "    key = point[0]\n",
        "    words = tokenize(point[1])\n",
        "    matrix = np.array( [w2v[w] for w in words if w in w2v] )\n",
        "    features = np.zeros(k)\n",
        "    for v in matrix:\n",
        "        c = model.predict(v)\n",
        "        features[c] += 1\n",
        "    return (key, features)\n",
        "    \n",
        "quantRDD = dataRDD.map(lambda x: quantizador(x, modelK, 500, w2v))\n",
        "\n",
        "print (quantRDD.take(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "YoAfB2_j72e_"
      },
      "outputs": [],
      "source": [
        "# TEST Implement a TF function (2a)\n",
        "assert quantRDD.take(1)[0][1].sum() == 5, 'valores incorretos'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Laboratorio04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}